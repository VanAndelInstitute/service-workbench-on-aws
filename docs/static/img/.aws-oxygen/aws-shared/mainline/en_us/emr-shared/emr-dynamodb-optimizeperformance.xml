<?xml version="1.0" encoding="utf-8" standalone="no"?>
<!DOCTYPE section PUBLIC "-//OASIS//DTD DocBook XML V4.5//EN" "file://zonbook/docbookx.dtd"  
[
<!ENTITY % phrases-shared SYSTEM "file://AWSShared/common/phrases-shared.ent">
%phrases-shared;   
<!ENTITY % xinclude SYSTEM "file://AWSShared/common/xinclude.mod">
%xinclude;
]>


 
    
<section id="EMR_Hive_Optimizing"  role="topic">
    <title id="EMR_Hive_Optimizing.title">Optimizing Performance for &EMR; Operations in &DDB;</title>
    <sectioninfo>
        <abstract>
            <para>Follow these tips to get the most out of &EMR; performance when integrating with &DDBlong;.</para>
        </abstract>
    </sectioninfo>
    <titleabbrev>Optimizing Performance</titleabbrev>
    <para>
        &EMR; operations on a &DDB; table count as read operations, and are subject
        to the table's provisioned throughput settings. &EMR; implements its own logic to
        try to balance the load on your &DDB; table to minimize the possibility of exceeding
        your provisioned throughput. At the end of each Hive query, &EMR; returns information 
        about the &cluster; used to process the query, including how
        many times your provisioned throughput was exceeded. You can use this information, as well 
        as &CW; metrics about your &DDB; throughput, to better
        manage the load on your &DDB; table in subsequent requests.  
    </para>
    <para>
        The following factors
        influence Hive query performance when working with &DDB; tables.
    </para>
    <section id="ProvisionedReadCapacityUnits">
        <title id="ProvisionedReadCapacityUnits.title">Provisioned Read Capacity Units</title>
        <para> When you run Hive queries against a &DDB; table, you need to ensure that
            you have provisioned a sufficient amount of read capacity units. </para>
        <para> For example, suppose that you have provisioned 100 units of Read Capacity for your
            DynamoDB table. This will let you perform 100 reads, or 409,600 bytes, per second. If
            that table contains 20GB of data (21,474,836,480 bytes), and your Hive query performs a
            full table scan, you can estimate how long the query will take to run: </para>
        <para>
            <emphasis> 21,474,836,480 / 409,600 = 52,429 seconds = 14.56 hours </emphasis>
        </para>
        <para> The only way to decrease the time required would be to adjust the read capacity units
            on the source DynamoDB table. Adding more nodes to the &EMR; &cluster; will not help. </para>
        <para> In the Hive output, the completion percentage is updated when one or more mapper
            processes are finished. For a large DynamoDB table with a low provisioned Read Capacity
            setting, the completion percentage output might not be updated for a long time; in the
            case above, the job will appear to be 0% complete for several hours. For more detailed
            status on your job's progress, go to the &EMR; console; you will be able to
            view the individual mapper task status, and statistics for data reads. </para>
        <para> You can also log on to Hadoop interface on the master node and see the Hadoop
            statistics. This will show you the individual map task status and some data read
            statistics. For more information, see the following topics: </para>
        <itemizedlist>
            <listitem>
                <para>
                    <ulink
                        url="&url-emr-dev;emr-web-interfaces.html"
                        >Web Interfaces Hosted on the Master Node</ulink>
                </para>
            </listitem>
            <listitem>
                <para>
                    <ulink
                        url="&url-emr-dev;UsingtheHadoopUserInterface.html"
                        >View the Hadoop Web Interfaces</ulink>
                </para>
            </listitem>
        </itemizedlist>
    </section>
    
    <section id="ReadPercent">
        <title id="ReadPercent.title">Read Percent Setting</title>
        <para>
            By default, &EMR; manages the request load against your &DDB; table according
            to your current provisioned throughput. However, when &EMR; returns information
            about your job that includes a high number of provisioned throughput exceeded
            responses, you can adjust the default read rate using the
            <parameter>dynamodb.throughput.read.percent</parameter> parameter when you set
            up the Hive table. For more information about setting the read percent parameter, see 
            <ulink
                url="&url-emr-dev;EMR_Interactive_Hive.html#EMR_Hive_Options">Hive Options</ulink> in the <emphasis>&EMR; Developer Guide</emphasis>.
        </para>
    </section>
   
    <section id="WritePercent">
        <title id="WritePercent.title">Write Percent Setting</title>
        <para>
            By default, &EMR; manages the request load against your &DDB; table according
            to your current provisioned throughput. However, when &EMR; returns information
            about your job that includes a high number of provisioned throughput exceeded
            responses, you can adjust the default write rate using the
            <parameter>dynamodb.throughput.write.percent</parameter> parameter when you set
            up the Hive table. For more information about setting the write percent parameter, see 
            <ulink
                url="&url-emr-dev;EMR_Interactive_Hive.html#EMR_Hive_Options">Hive Options</ulink> in the <emphasis>&EMR; Developer Guide</emphasis>.
        </para>
    </section>
    
    
    <section id="emr-ddb-retry-duration">
        <title id="emr-ddb-retry-duration.title">Retry Duration Setting</title>
        <para>
            By default, &EMR; re-runs a Hive query if it has not returned a result 
            within two minutes, the default retry interval. You can adjust this interval by setting 
            the <code>dynamodb.retry.duration</code> parameter when you run a Hive query. 
            For more information about setting the write percent parameter, see 
            <ulink
                url="&url-emr-dev;EMR_Interactive_Hive.html#EMR_Hive_Options">Hive Options</ulink> in the <emphasis>&EMR; Developer Guide</emphasis>.
        </para>
    </section>
    
    <section id="NumberMapTasks">
        <title id="NumberMapTasks.title">Number of Map Tasks</title>
        <para>
            The mapper daemons that Hadoop launches to process your requests 
            to export and query data stored in &DDB; are capped at a 
            maximum read rate of 1 MiB per second to limit the read capacity used. 
            If you have additional provisioned throughput available on 
            &DDB;, you can improve the performance of Hive export and query operations by 
            increasing the number of mapper daemons. To do this, you can either increase the 
            number of EC2 instances in your &cluster; <emphasis>or</emphasis>  
            increase the number of mapper daemons running on each EC2 instance.
        </para>
        
        <para>
            You can increase the number of EC2 instances in a &cluster; by 
            stopping the current &cluster; and re-launching it with 
            a larger number of EC2 instances. You specify the number of EC2 instances 
            in the <guilabel>Configure EC2 Instances</guilabel> dialog box if you're 
            launching the &cluster; from the &EMR; console, 
            or with the <code>--num-instances</code> option if you're launching the &cluster; from the CLI.
        </para>
        
        <para>
            The number of map tasks run on an instance depends on the EC2 instance type. For more information about the supported EC2 instance types and the number of mappers each one 
            provides, go to <!-- <ulink url="&url-emr-dev;TaskConfiguration_AMI2.html">Task Configuration</ulink> --> <ulink url="&url-emr-dev;emr-hadoop-config.html">Hadoop Configuration Reference</ulink> in the <emphasis>&EMR; Developer Guide</emphasis>. There, you will find a "Task Configuration" section for each of the supported configurations.
        </para>
        
        <para>
            Another way to increase the number of mapper daemons is to change the 
            <code>mapred.tasktracker.map.tasks.maximum</code> configuration parameter of Hadoop to a higher value.
            This has the advantage of giving you more mappers without increasing either the number or the size of EC2 instances, which saves you money. 
            A disadvantage is that setting this value too high can cause the EC2 instances in your &cluster; to run out of memory. 
            To set <code>mapred.tasktracker.map.tasks.maximum</code>, launch the &cluster; and specify the Configure Hadoop bootstrap action, passing in 
            a value for <code>mapred.tasktracker.map.tasks.maximum</code> as one of the arguments of the bootstrap action. This is shown in the following example.</para>
        
        <programlisting format="linespecific">
--bootstrap-action s3n://elasticmapreduce/bootstrap-actions/configure-hadoop \
  --args -m,mapred.tasktracker.map.tasks.maximum=10 
        </programlisting>
        
        <para>For more information about 
            bootstrap actions, see <ulink url="&url-emr-dev;Bootstrap.html">Using Custom Bootstrap Actions</ulink> in the <emphasis>&EMR; Developer Guide</emphasis>. 
        </para>

    </section>
    
    
    <section id="ParallelDataRequests">
        <title id="ParallelDataRequests.title">Parallel Data Requests</title>
        
        <para>
            Multiple data requests, either from more than one user or more than one
            application to a single table may drain read provisioned throughput and slow performance.
        </para>
    </section>
    
    <section id="ProcessDuration">
        <title id="ProcessDuration.title">Process Duration</title>
        
        <para>
            Data consistency in &DDB; depends on the order of read and write operations
            on each node. While a Hive query is in progress, another application
            might load new data into the  &DDB; table or modify or delete existing data. In this case, the results of 
            the Hive query might not reflect changes made to the data while the query was running.
        </para>
        
    </section>
    
    <section id="AvoidExceedingThroughput">
        <title id="AvoidExceedingThroughput.title">Avoid Exceeding Throughput</title>
        <para>
            When running Hive queries against &DDB;, take care not to exceed your provisioned throughput, because this 
            will deplete capacity needed for your application's calls to <code>DynamoDB::Get</code>. To ensure that this 
            is not occurring, you should regularly monitor the read volume and throttling on application calls to <code>DynamoDB::Get</code> 
            by checking logs and monitoring metrics in Amazon CloudWatch.
        </para>
    </section>
    
    <section id="RequestTime">
        <title id="RequestTime.title">Request Time</title>
        
        <para>
            Scheduling Hive queries that access a &DDB; table when there is 
            lower demand on the &DDB; table improves performance. For example, if most of 
            your application's users live in San Francisco, you might choose to export daily 
            data at 4 a.m. PST, when the majority of users are asleep, and not updating records in your &DDB; database.
        </para>
        
    </section>
    
    <section id="TimeBasedTables">
        <title id="TimeBasedTables.title">Time-Based Tables</title>
        
        <para>
            If the data is organized as a series of time-based &DDB; tables, such as one table per day, 
            you can export the data when the table becomes no longer active. You can use this technique 
            to back up data to &S3; on an ongoing fashion.
        </para>
        
    </section>
    
    <section id="ArchivedData">
        <title id="ArchivedData.title">Archived Data</title>
        <para>
            If you plan to run many Hive queries against the data stored in &DDB; and your application 
            can tolerate archived data, you may want to export the data to HDFS or &S3; and run the Hive queries 
            against a copy of the data instead of &DDB;. This conserves your read operations and provisioned 
            throughput.
        </para>
    </section>
    
    <section id="ViewingHadoopLogs">
        <title id="ViewingHadoopLogs.title">Viewing Hadoop Logs</title>
        <para>
            If you run into an error, you can investigate what went wrong by viewing the Hadoop logs and user interface. For more information, see 
            <ulink url="&url-emr-dev;UsingSSHtoMonitorJobStatus.html">How to Monitor Hadoop on a Master Node</ulink> and 
            <ulink url="&url-emr-dev;UsingtheHadoopUserInterface.html">How to Use the Hadoop User Interface</ulink> in the <emphasis>&EMR; Developer Guide</emphasis>.
        </para>
    </section>
</section>

